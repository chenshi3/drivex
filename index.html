<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DriveX: Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DriveX: Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">DriveX: Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving</h1>
          <h1 class="title is-1 publication-title">ICCV 2025</h1>
          <div class="is-size-4 publication-authors">
            <div class="author-row">
              <span class="author-block" style="font-size: 28px;">
              </span>
            </div>
            <div class="author-row">
              <span class="author-block">
                Chen Shi<sup>1,2</sup>,</span>
              <span class="author-block">
                Shaoshuai Shi<sup>2</sup>,</span>
              <span class="author-block">
                Kehua Sheng<sup>2</sup>,</span>
              <span class="author-block">
                Bo Zhang<sup>2</sup>,</span>
              <span class="author-block">
                Li Jiang<sup>1</sup></span>
            </div>
          </div>

          <div class="is-size-4 publication-authors">
            <span class="author-block"><sup>1</sup>The Chinese University of Hong Kong, Shenzhen,</span>
            <span class="author-block"><sup>2</sup>Voyager Research, Didi Chuxing</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2505.19239"
                   class="external-link button is-normal is-rounded is-dark"
                   style="font-size: 18px; padding: 12px 24px;">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section style="margin-top: -6rem;">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified" style="font-size: 20px;">
          <p>
            Data-driven learning has advanced autonomous driving, yet task-specific models struggle with out-of-distribution scenarios due to their narrow optimization objectives and reliance on costly annotated data. We present DriveX, a self-supervised world model that learns generalizable scene dynamics and holistic representations (geometric, semantic, and motion) from large-scale driving videos. DriveX introduces Omni Scene Modeling (OSM), a module that unifies multimodal supervision—3D point cloud forecasting, 2D semantic representation, and image generation—to capture comprehensive scene evolution. To simplify learning complex dynamics, we propose a decoupled latent world modeling strategy that separates world representation learning from future state decoding, augmented by dynamic-aware ray sampling to enhance motion modeling. For downstream adaptation, we design Future Spatial Attention (FSA), a unified paradigm that dynamically aggregates spatiotemporal features from DriveX's predictions to enhance task-specific inference. Extensive experiments demonstrate DriveX's effectiveness: it achieves significant improvements in 3D future point cloud prediction over prior work, while attaining state-of-the-art results on diverse tasks including occupancy prediction, flow estimation, and end-to-end driving. These results validate DriveX's capability as a general-purpose world model, paving the way for robust and unified autonomous driving frameworks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
      </div>
</section>

<section class="section" >
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview of DriveX</h2>
        <img src="method.png" alt="framework">
        <div class="content has-text-justified" style="font-size: 18.5px;">
          <p>
            <b>Top:</b> An overview of our DriveX framework. The learning process consists of two stages: world representation learning, where the model learns temporal and geometric semantics through Omni Scene Modeling, and latent future decoding, where the model predicts future states in the learned latent space. Both stages are trained in a self-supervised manner. <b>Bottom:</b> Illustration of the FSA paradigm. Various driving tasks can dynamically aggregate information from predicted latent features through FSA. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Method. -->
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is constructed using the templet provided by <a
                href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. Thanks for their effort.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
